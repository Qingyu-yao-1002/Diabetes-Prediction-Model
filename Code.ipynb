{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes prediction model project: data processing, model training and business suggestion generation\n",
    "This code implements a complete diabetes prediction model project, covering the whole process from data loading to business proposal generation. The project aims to establish an effective diabetes prediction model and provide valuable suggestions for related businesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is mainly used to import various libraries and modules required for the project, preparing for subsequent data analysis, modeling, and visualization work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part I :Read the CSV file to obtain the diabetes dataset. The basic information of the output data, such as data type, column name, and missing value status, is included. Print out the number of rows and columns in the dataset for users to quickly understand the size of the data. Help us identify potential issues in the data in advance, such as a small amount of data or insufficient number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data loading and exploration\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load diabetes dataset\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Basic data information:\")\n",
    "    df.info()\n",
    "    \n",
    "    # Display the number of rows and columns in the dataset\n",
    "    rows, columns = df.shape\n",
    "    \n",
    "    if rows < 500:\n",
    "        print(\"Warning: If the number of rows in the dataset is less than 500, it may cause overfitting of the model\")\n",
    "    if columns < 10:\n",
    "        print(\"Warning: If the number of columns in the dataset is less than 10, there may be insufficient features\")\n",
    "    \n",
    "    # Check the number of rows and columns in the data\n",
    "    if rows < 30:\n",
    "        raise ValueError(\"Error: The number of rows in the dataset is less than 30, and the amount of data is too small to model\")\n",
    "    \n",
    "    # View the number of rows and columns in the dataset\n",
    "    print(f\"Number of rows in the dataset:{rows}，Number of columns:{columns}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II :A function named preprocess_data is defined. Its core function is to preprocess the diabetes data set to prepare for subsequent machine learning modeling.\n",
    "1. Identify and process invalid zero values in medical indicators, and convert them into missing values.\n",
    "2. Use median imputation to handle missing values and ensure data integrity.\n",
    "3. Create new classification features through binning operations to enhance the expressive power of data.\n",
    "4. Encode the classification features so that they can be effectively processed by machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Data preprocessing: handling missing values, outliers, and feature engineering\"\"\"\n",
    "    # copy data\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handling missing values (assuming 0 is a missing value, except for the Pregnant and Outcome columns)\n",
    "    columns_to_replace = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "    for col in columns_to_replace:\n",
    "        df_processed[col] = df_processed[col].replace(0, np.nan)\n",
    "    \n",
    "    # Calculate the proportion of missing values\n",
    "    missing_values = df_processed.isnull().sum()\n",
    "    print(\"\\nMissing value statistics:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    \n",
    "    # Handling missing values - using median padding\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_processed[columns_to_replace] = imputer.fit_transform(df_processed[columns_to_replace])\n",
    "    \n",
    "    # Feature Engineering: Creating New Features\n",
    "    df_processed['AgeGroup'] = pd.cut(df_processed['Age'], bins=[0, 30, 45, 60, 100], labels=['Youth', 'Middle aged', 'Middle aged', 'Elderly', 'Elderly'])\n",
    "    df_processed['BMI_Category'] = pd.cut(df_processed['BMI'], bins=[0, 18.5, 25, 30, 100], labels=['underweight', 'normal', 'overweight', 'obese'])\n",
    "    \n",
    "    # Coding classification features\n",
    "    df_processed = pd.get_dummies(df_processed, columns=['AgeGroup', 'BMI_Category'], drop_first=True)\n",
    "    \n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part III :This code defines a function called visualize_data, whose main function is to visually analyze the diabetes dataset, helping us to deeply understand the data characteristics and their relationship with the target variables.\n",
    "1. Correlation heatmap: It visually displays the correlation between various features, helping us discover multicollinearity and key predictive features.\n",
    "2. Box plot: Compare the distribution differences of features between patients and non patients, providing a basis for feature engineering and model interpretation.\n",
    "3. Target variable distribution: Check if the categories are balanced to avoid model bias caused by imbalanced data.\n",
    "4. Feature importance ranking: quantify the prediction ability of each feature to diabetes, and provide reference for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Visualization and Analysis\n",
    "def visualize_data(df):\n",
    "    \"\"\"Data Visualization and Analysis\"\"\"\n",
    "    # Set image clarity\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    \n",
    "    # Draw feature correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    corr = df.corr()\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "    plt.title(\"Feature correlation heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlation_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Draw feature distribution and box plot\n",
    "    numeric_features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(numeric_features):\n",
    "        sns.boxplot(x='Outcome', y=feature, data=df, ax=axes[i])\n",
    "        axes[i].set_title(f'{feature} distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Draw the distribution of target variables\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x='Outcome', data=df)\n",
    "    plt.title('Distribution of diabetes patients')\n",
    "    plt.savefig('target_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate and print feature importance (based on correlation)\n",
    "    print(\"\\nCorrelation between characteristics and diabetes:\")\n",
    "    print(corr['Outcome'].sort_values(ascending=False)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part IV :This code defines a function called train_model, whose core function is to conduct feature selection, model training and evaluation to build a diabetes prediction model.\n",
    "1. Data partitioning: Divide the data into training and testing sets to ensure category balance.\n",
    "2. Automated process: Integrating pre-processing, feature selection, and model training through pipelines.\n",
    "3. Hyperparameter optimization: Use grid search and cross validation to find the optimal model configuration.\n",
    "4. Multi model comparison: Train logistic regression and SVM models simultaneously, and evaluate performance from multiple dimensions.\n",
    "5. Result visualization: Visualize the model performance through ROC curve, precision recall curve, and feature importance map.\n",
    "The final returned best_madels and results objects contain the optimal configurations and evaluation results of all models, providing data support for subsequent model selection and business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature selection and model training\n",
    "def train_model(df):\n",
    "    \"\"\"Feature selection, model training, and evaluation\"\"\"\n",
    "    # Prepare features and target variables\n",
    "    X = df.drop(['Outcome'], axis=1)\n",
    "    y = df['Outcome']\n",
    "    \n",
    "    # Divide the training set and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]}，Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Create feature selector\n",
    "    feature_selector = SelectKBest(score_func=f_classif, k=8)\n",
    "    \n",
    "    # Create a standardization tool\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Create model pipeline\n",
    "    pipelines = {\n",
    "        'Logistic Regression': Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', scaler),\n",
    "            ('feature_selector', feature_selector),\n",
    "            ('classifier', LogisticRegression(random_state=42))\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', scaler),\n",
    "            ('feature_selector', feature_selector),\n",
    "            ('classifier', SVC(random_state=42, probability=True))\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    # Set hyperparameter grid\n",
    "    param_grids = {\n",
    "        'Logistic Regression': {\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "            'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "        },\n",
    "        'SVM': {\n",
    "            'classifier__C': [0.1, 1, 10, 100],\n",
    "            'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'classifier__gamma': ['scale', 'auto']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # cross validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Training and Evaluating Models\n",
    "    best_models = {}\n",
    "    results = {}\n",
    "    \n",
    "    for name, pipeline in pipelines.items():\n",
    "        print(f\"\\nTrain the {name} model...\")\n",
    "        \n",
    "        # grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline, \n",
    "            param_grids[name], \n",
    "            cv=cv, \n",
    "            scoring='accuracy',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # training model\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Save the best model\n",
    "        best_models[name] = grid_search.best_estimator_\n",
    "        \n",
    "        # evaluation model\n",
    "        y_pred = best_models[name].predict(X_test)\n",
    "        y_prob = best_models[name].predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate evaluation indicators\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Calculate the precision recall curve\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "        \n",
    "        # Save the Results\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': cm,\n",
    "            'report': report,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'roc_auc': roc_auc,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} optimum parameter: {grid_search.best_params_}\")\n",
    "        print(f\"{name} accuracy: {accuracy:.4f}\")\n",
    "        print(f\"{name} confusion matrix:\\n{cm}\")\n",
    "        print(f\"{name} Classification report:\\n{report}\")\n",
    "    \n",
    "    # Draw ROC curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result in results.items():\n",
    "        plt.plot(result['fpr'], result['tpr'], lw=2, label=f'{name} (AUC = {result[\"roc_auc\"]:.3f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('false positive rate')\n",
    "    plt.ylabel('true positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig('roc_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Draw precision recall curve\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result in results.items():\n",
    "        plt.plot(result['recall'], result['precision'], lw=2, label=f'{name}')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('recall rate')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy recall curve')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.savefig('precision_recall_curve.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance analysis (applicable only to logistic regression)\n",
    "    if 'Logistic Regression' in best_models:\n",
    "        lr_model = best_models['Logistic Regression'].named_steps['classifier']\n",
    "        feature_names = X.columns[best_models['Logistic Regression'].named_steps['feature_selector'].get_support()]\n",
    "        coefficients = lr_model.coef_[0]\n",
    "        \n",
    "        # Create Feature Importance DataFrame\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': coefficients,\n",
    "            'Importance': np.abs(coefficients)\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nFeature Importance Analysis (Logistic Regression):\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        # Draw a feature importance map\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "        plt.title('Feature importance (absolute value of logistic regression coefficient)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.close()\n",
    "    \n",
    "    return best_models, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part V :This code defines a function called generate_insights, whose core function is to generate business insights and recommendations based on the model training results. This function implements the transformation from machine learning models to business decisions:\n",
    "1. Model interpretation: Extracting meaningful business insights by analyzing the performance of the best model.\n",
    "2. Decision support: Based on the model results, propose specific action suggestions to help the medical team optimize the screening process.\n",
    "3. Knowledge accumulation: Save the analysis conclusions in text form for cross departmental communication and long-term reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Explanation and Business Suggestions\n",
    "def generate_insights(results, best_models, df):\n",
    "    \"\"\"Generate model explanations and business recommendations\"\"\"\n",
    "    # Find the best model\n",
    "    best_model_name = max(results, key=lambda k: results[k]['accuracy'])\n",
    "    best_accuracy = results[best_model_name]['accuracy']\n",
    "    \n",
    "    print(f\"\\nbest model: {best_model_name}，accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate insights\n",
    "    insights = [\n",
    "        f\"1. The best model is {best_model_name}, with an accuracy of {best_accuracy:.2%}, indicating that the model has good predictive ability.\",\n",
    "        \"2. Glucose, BMI and diabetes genetic function are the most important factors to predict diabetes.\",\n",
    "        \"3. From the confusion matrix, the model still has room for improvement in identifying diabetes patients (positive), and more positive samples can be collected.\",\n",
    "        \"4. This model can serve as a preliminary screening tool to help doctors identify high-risk patients, but the final diagnosis still needs to be combined with clinical symptoms.\"\n",
    "    ]\n",
    "    \n",
    "    # Generate suggestions\n",
    "    recommendations = [\n",
    "        \"1. Carry out prevention and publicity activities targeting high-risk populations (high BMI, high blood sugar levels) to promote a healthy lifestyle.\",\n",
    "        \"2. Collect more sample data, especially diabetes patient data, to improve model performance.\",\n",
    "        \"3. Consider using ensemble learning methods or deep learning to further improve prediction accuracy.\",\n",
    "        \"4. Develop a simple application that allows doctors to enter patient data and obtain diabetes risk predictions.\"\n",
    "    ]\n",
    "    \n",
    "    # Save insights and suggestions to a file\n",
    "    with open('insights_and_recommendations.txt', 'w') as f:\n",
    "        f.write(\"### Model Insights ###\\n\")\n",
    "        for insight in insights:\n",
    "            f.write(f\"- {insight}\\n\")\n",
    "        \n",
    "        f.write(\"\\n### Business Suggestions ###\\n\")\n",
    "        for recommendation in recommendations:\n",
    "            f.write(f\"- {recommendation}\\n\")\n",
    "    \n",
    "    print(\"\\nInsights and suggestions have been generated to insights_and_decommendations.txt\")\n",
    "    \n",
    "    return insights, recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part VI :This code defines the main function of the project, which is to sequentially call various functional modules and connect the complete project process from data loading to generating business suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "def main():\n",
    "    print(\"===== Diabetes prediction model project =====\")\n",
    "    \n",
    "    # 1. Load data\n",
    "    df = load_data('diabetes.csv')\n",
    "    \n",
    "    # 2. Data preprocessing\n",
    "    df_processed = preprocess_data(df)\n",
    "    \n",
    "    # 3. Data Visualization and Analysis\n",
    "    visualize_data(df_processed)\n",
    "    \n",
    "    # 4. Model Training and Evaluation\n",
    "    best_models, results = train_model(df_processed)\n",
    "    \n",
    "    # 5. Generate insights and recommendations\n",
    "    insights, recommendations = generate_insights(results, best_models, df_processed)\n",
    "    \n",
    "    print(\"\\n===== Project Completion =====\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
